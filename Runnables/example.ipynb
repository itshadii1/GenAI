{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d7b7436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class NakliLLM:\n",
    "\n",
    "  def __init__(self):\n",
    "    print('LLM created')\n",
    "\n",
    "  def predict(self, prompt):\n",
    "\n",
    "    response_list = [\n",
    "        'Delhi is the capital of India',\n",
    "        'IPL is a cricket league',\n",
    "        'AI stands for Artificial Intelligence'\n",
    "    ]\n",
    "\n",
    "    return {'response': random.choice(response_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7f4c789",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NakliPromptTemplate:\n",
    "\n",
    "  def __init__(self, template, input_variables):\n",
    "    self.template = template\n",
    "    self.input_variables = input_variables\n",
    "\n",
    "  def format(self, input_dict):\n",
    "    return self.template.format(**input_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da893b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = NakliPromptTemplate(\n",
    "    template='Write a {length} poem about {topic}',\n",
    "    input_variables=['length', 'topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fa6dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.format({'length':'short','topic':'india'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e776333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM created\n"
     ]
    }
   ],
   "source": [
    "llm = NakliLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9b14071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': 'AI stands for Artificial Intelligence'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fca1750",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NakliLLMChain:\n",
    "\n",
    "  def __init__(self, llm, prompt):\n",
    "    self.llm = llm\n",
    "    self.prompt = prompt\n",
    "\n",
    "  def run(self, input_dict):\n",
    "\n",
    "    final_prompt = self.prompt.format(input_dict)\n",
    "    result = self.llm.predict(final_prompt)\n",
    "\n",
    "    return result['response']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e11fbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = NakliPromptTemplate(\n",
    "    template='Write a {length} poem about {topic}',\n",
    "    input_variables=['length', 'topic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64f86368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM created\n"
     ]
    }
   ],
   "source": [
    "llm = NakliLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee6f5271",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = NakliLLMChain(llm, template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35a98c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI stands for Artificial Intelligence'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run({'length':'short', 'topic': 'india'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35f86824",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mollama\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 1) Spin up your Ollama server (in your shell):\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#    ollama serve\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 2) In Python, point LangChain at that server:\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Initialize the Ollama-backed chat model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# replace with your Ollama model name\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp://127.0.0.1:11434\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# default Ollama serve endpoint\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 3) Define any \"tools\" (i.e. functions your agent can call)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwikipedia_search\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# dummy implementation — swap in your own search client\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "# First install the needed packages:\n",
    "# pip install langchain langchain-community ollama\n",
    "\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "import ollama\n",
    "\n",
    "# 1) Spin up your Ollama server (in your shell):\n",
    "#    ollama serve\n",
    "#\n",
    "# 2) In Python, point LangChain at that server:\n",
    "\n",
    "# Initialize the Ollama-backed chat model\n",
    "model = ollama(\n",
    "    model=\"llama2\",                 # replace with your Ollama model name\n",
    "    base_url=\"http://127.0.0.1:11434\",  # default Ollama serve endpoint\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 3) Define any \"tools\" (i.e. functions your agent can call)\n",
    "\n",
    "def wikipedia_search(query: str) -> str:\n",
    "    # dummy implementation — swap in your own search client\n",
    "    return f\"Results for '{query}' from Wikipedia...\"\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"WikipediaSearch\",\n",
    "        func=wikipedia_search,\n",
    "        description=\"Useful for looking up facts on Wikipedia\"\n",
    "    ),\n",
    "    # add more Tool(...) objects here as needed\n",
    "]\n",
    "\n",
    "# 4) Create and run the agent\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    model,\n",
    "    agent=\"zero-shot-react-description\",  # or \"conversational-react-description\"\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"Who wrote *Les Misérables* and what year was it first published?\"\n",
    "    result = agent.run(prompt)\n",
    "    print(\"\\n––– Agent Response –––\\n\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c035b5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Ollama' from 'langchain.chat_models' (/Users/hadi/.pyenv/versions/3.10.11/lib/python3.10/site-packages/langchain/chat_models/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Ollama\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tool, initialize_agent\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 1) Point at your running Ollama server:\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Ollama' from 'langchain.chat_models' (/Users/hadi/.pyenv/versions/3.10.11/lib/python3.10/site-packages/langchain/chat_models/__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import Ollama\n",
    "from langchain.agents import Tool, initialize_agent\n",
    "\n",
    "# 1) Point at your running Ollama server:\n",
    "llm = Ollama(\n",
    "    model=\"llama2\",                     # or whatever model name you’ve pulled\n",
    "    base_url=\"http://127.0.0.1:11434\",  # Ollama’s default serve endpoint\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 2) Wrap any helper functions as Tools:\n",
    "def wiki_search(q: str) -> str:\n",
    "    # replace with real API calls if you like\n",
    "    return f\"🔍 searched Wikipedia for: {q}\"\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"wiki_search\",\n",
    "        func=wiki_search,\n",
    "        description=\"Use to look up factual info on Wikipedia\"\n",
    "    ),\n",
    "    # …add more tools here…\n",
    "]\n",
    "\n",
    "# 3) Initialize the agent:\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=\"zero-shot-react-description\",  # lets the LLM decide when to call tools\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 4) Run it:\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"Who directed Inception, and what year did it come out?\"\n",
    "    answer = agent.run(prompt)\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9db5eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQuestion: Who discovered penicillin and when?\n",
      "Thought: I need to find out about Alexander Fleming's discovery of penicillin.\n",
      "Action: WikipediaSearch\n",
      "Action Input: 'Alexander Fleming penicillin'\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m(Fake Wikipedia Search Result for: 'Alexander Fleming penicillin')\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: Who discovered penicillin and when?\n",
      "Thought: Question: Who discovered penicillin and when?\n",
      "Thought: I need to find out about Alexander Fleming's discovery of penicillin.\n",
      "Action: WikipediaSearch\n",
      "Action Input: 'Alexander Fleming penicillin'\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m(Fake Wikipedia Search Result for: 'Alexander Fleming penicillin')\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: Who discovered penicillin and when?\n",
      "Thought: Question: Who discovered penicillin and when?\n",
      "Thought: I need to find out about Alexander Fleming's discovery of penicillin.\n",
      "Action: WikipediaSearch\n",
      "Action Input: 'Alexander Fleming penicillin'\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m(Fake Wikipedia Search Result for: 'Alexander Fleming penicillin')\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: Who discovered penicillin and when?\n",
      "Thought: I need to find out about Alexander Fleming's discovery of penicillin.\n",
      "Action: WikipediaSearch\n",
      "Action Input: 'Alexander Fleming penicillin'\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m(Fake Wikipedia Search Result for: 'Alexander Fleming penicillin')\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: Alexander Fleming discovered penicillin in 1928.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Result:\n",
      " Alexander Fleming discovered penicillin in 1928.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.agents import Tool, initialize_agent\n",
    "\n",
    "# 1) Point at your local Ollama instance\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2:latest\",                     # Use model you've pulled via `ollama pull llama2`\n",
    "    base_url=\"http://localhost:11434\", # Default for ollama serve\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 2) Optional: Tools\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    return f\"(Fake Wikipedia Search Result for: {query})\"\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"WikipediaSearch\",\n",
    "        func=search_wikipedia,\n",
    "        description=\"Search Wikipedia for general knowledge\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# 3) Create the agent\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 4) Run it\n",
    "if __name__ == \"__main__\":\n",
    "    prompt = \"Who discovered penicillin and when?\"\n",
    "    result = agent.run(prompt)\n",
    "    print(\"\\nResult:\\n\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f23887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
